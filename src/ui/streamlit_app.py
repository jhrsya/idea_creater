"""
Streamlitç”¨æˆ·ç•Œé¢ - ä¸€é”®å¼åˆ›æ–°ç‚¹ç”Ÿæˆå™¨
"""
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from pathlib import Path
import json
from typing import List, Dict
import sys
import os
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from config.settings import settings
from src.crawler.arxiv_crawler import ArXivCrawler
from src.parser.pdf_parser import PDFParser
from src.extractor.innovation_extractor import InnovationExtractor, ExtractedInnovations, InnovationPoint
from src.generator.idea_generator import IdeaGenerator


def main():
    """ä¸»åº”ç”¨å‡½æ•°"""
    st.set_page_config(
        page_title="æ™ºèƒ½è®ºæ–‡åˆ›æ–°ç‚¹ç”Ÿæˆå™¨",
        page_icon="ğŸ§ ",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # è‡ªå®šä¹‰CSSæ ·å¼
    st.markdown("""
    <style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        text-align: center;
        color: white;
    }
    
    .innovation-card {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        border-left: 4px solid #667eea;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .idea-card {
        background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        border-left: 4px solid #ff6b6b;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 8px;
        text-align: center;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }
    
    .progress-step {
        background: #e3f2fd;
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
        border-left: 3px solid #2196f3;
    }
    
    .success-step {
        background: #e8f5e8;
        border-left-color: #4caf50;
    }
    
    .error-step {
        background: #ffebee;
        border-left-color: #f44336;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # ä¸»æ ‡é¢˜
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ§  æ™ºèƒ½è®ºæ–‡åˆ›æ–°ç‚¹ç”Ÿæˆå™¨</h1>
        <p>ä¸€é”®æœç´¢è®ºæ–‡ï¼Œè‡ªåŠ¨æå–åˆ›æ–°ç‚¹ï¼Œç”Ÿæˆæ–°çš„ç ”ç©¶æ–¹å‘</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")
        
        # æ£€æŸ¥APIé…ç½®
        if not settings.DEEPSEEK_API_KEY:
            st.error("âš ï¸ æœªé…ç½®DeepSeek APIå¯†é’¥")
            st.info("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®DEEPSEEK_API_KEY")
            return
        else:
            st.success("âœ… APIé…ç½®æ­£å¸¸")
        
        st.divider()
        
        # æœç´¢è®¾ç½®
        max_papers = st.slider("æœ€å¤§è®ºæ–‡æ•°é‡", 3, 20, 5)
        st.info(f"å°†æœç´¢æœ€å¤š {max_papers} ç¯‡è®ºæ–‡")
        
        # å†å²è®°å½•
        st.header("ğŸ“š å†å²è®°å½•")
        show_history_sidebar()
    
    # ä¸»å†…å®¹
    tab1, tab2, tab3 = st.tabs(["ğŸš€ ä¸€é”®ç”Ÿæˆ", "ğŸ“Š ç»“æœåˆ†æ", "ğŸ“‹ è¯¦ç»†æµç¨‹"])
    
    with tab1:
        show_one_click_generation_page(max_papers)
    
    with tab2:
        show_analysis_page()
    
    with tab3:
        show_detailed_process_page()


def show_one_click_generation_page(max_papers: int):
    """ä¸€é”®ç”Ÿæˆé¡µé¢"""
    st.header("ğŸš€ ä¸€é”®ç”Ÿæˆåˆ›æ–°ç‚¹")
    
    # æœç´¢è¾“å…¥
    col1, col2 = st.columns([3, 1])
    
    with col1:
        search_query = st.text_input(
            "ğŸ” è¾“å…¥ç ”ç©¶ä¸»é¢˜",
            placeholder="ä¾‹å¦‚ï¼šreinforcement learning, transformer, computer vision",
            help="è¾“å…¥æ‚¨æ„Ÿå…´è¶£çš„ç ”ç©¶é¢†åŸŸå…³é”®è¯ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æœç´¢ç›¸å…³è®ºæ–‡å¹¶æå–åˆ›æ–°ç‚¹"
        )
    
    with col2:
        st.write("")  # å ä½ç¬¦
        generate_button = st.button("ğŸš€ å¼€å§‹ç”Ÿæˆ", type="primary", use_container_width=True)
    
    if generate_button and search_query:
        # åˆ›å»ºä¼šè¯ç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_dir = settings.DATA_DIR / f"session_{timestamp}"
        
        # æ‰§è¡Œå®Œæ•´æµç¨‹
        result = run_complete_pipeline(search_query, max_papers, session_dir)
        
        if result:
            st.success("ğŸ‰ åˆ›æ–°ç‚¹ç”Ÿæˆå®Œæˆï¼")
            
            # æ˜¾ç¤ºç»“æœ
            display_innovation_results(result)
            
            # ä¿å­˜ç»“æœåˆ°ä¼šè¯ç›®å½•
            save_session_results(result, session_dir, search_query)
        else:
            st.error("âŒ ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®")
    
    elif generate_button and not search_query:
        st.warning("âš ï¸ è¯·è¾“å…¥æœç´¢ä¸»é¢˜")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    st.markdown("---")
    st.subheader("ğŸ’¡ æœç´¢ç¤ºä¾‹")
    
    examples = [
        "machine learning interpretability",
        "natural language processing transformer",
        "computer vision object detection",
        "reinforcement learning robotics",
        "deep learning optimization"
    ]
    
    cols = st.columns(len(examples))
    for i, example in enumerate(examples):
        with cols[i]:
            if st.button(f"ğŸ” {example}", key=f"example_{i}"):
                st.session_state.search_query = example
                st.rerun()


def run_complete_pipeline(search_query: str, max_papers: int, session_dir: Path) -> Dict | None:
    """è¿è¡Œå®Œæ•´çš„è®ºæ–‡åˆ†ææµç¨‹"""
    
    # åˆ›å»ºç›®å½•ç»“æ„
    papers_dir = session_dir / "papers"
    extracted_dir = session_dir / "extracted"
    innovations_dir = session_dir / "innovations"
    results_dir = session_dir / "results"
    
    for directory in [papers_dir, extracted_dir, innovations_dir, results_dir]:
        directory.mkdir(parents=True, exist_ok=True)
    
    # è¿›åº¦è¿½è¸ª
    progress_container = st.container()
    
    with progress_container:
        st.markdown("### ğŸ“‹ å¤„ç†è¿›åº¦")
        
        # æ­¥éª¤1: æœç´¢è®ºæ–‡
        step1_placeholder = st.empty()
        step1_placeholder.markdown('<div class="progress-step">ğŸ” æ­¥éª¤1: æœç´¢è®ºæ–‡ä¸­...</div>', unsafe_allow_html=True)
        
        try:
            crawler = ArXivCrawler()
            crawler.papers_dir = papers_dir
            
            papers = crawler.search_papers(search_query, max_papers)
            
            if not papers:
                step1_placeholder.markdown('<div class="progress-step error-step">âŒ æ­¥éª¤1: æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡</div>', unsafe_allow_html=True)
                return None
            
            step1_placeholder.markdown(f'<div class="progress-step success-step">âœ… æ­¥éª¤1: æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡</div>', unsafe_allow_html=True)
            
            # æ˜¾ç¤ºè®ºæ–‡ä¿¡æ¯
            with st.expander(f"ğŸ“„ æŸ¥çœ‹æ‰¾åˆ°çš„ {len(papers)} ç¯‡è®ºæ–‡"):
                for i, paper in enumerate(papers, 1):
                    st.write(f"**{i}. {paper.title}**")
                    st.write(f"ä½œè€…: {', '.join(paper.authors)}")
                    st.write(f"æ‘˜è¦: {paper.abstract[:200]}...")
                    st.write("---")
            
        except Exception as e:
            step1_placeholder.markdown(f'<div class="progress-step error-step">âŒ æ­¥éª¤1: æœç´¢å¤±è´¥ - {str(e)}</div>', unsafe_allow_html=True)
            return None
        
        # æ­¥éª¤2: ä¸‹è½½è®ºæ–‡
        step2_placeholder = st.empty()
        step2_placeholder.markdown('<div class="progress-step">ğŸ“¥ æ­¥éª¤2: ä¸‹è½½è®ºæ–‡ä¸­...</div>', unsafe_allow_html=True)
        
        try:
            downloaded = crawler.download_papers(papers)
            
            if not downloaded:
                step2_placeholder.markdown('<div class="progress-step error-step">âŒ æ­¥éª¤2: è®ºæ–‡ä¸‹è½½å¤±è´¥</div>', unsafe_allow_html=True)
                return None
            
            step2_placeholder.markdown(f'<div class="progress-step success-step">âœ… æ­¥éª¤2: æˆåŠŸä¸‹è½½ {len(downloaded)} ç¯‡è®ºæ–‡</div>', unsafe_allow_html=True)
            
        except Exception as e:
            step2_placeholder.markdown(f'<div class="progress-step error-step">âŒ æ­¥éª¤2: ä¸‹è½½å¤±è´¥ - {str(e)}</div>', unsafe_allow_html=True)
            return None
        
        # æ­¥éª¤3: è§£æè®ºæ–‡
        step3_placeholder = st.empty()
        step3_placeholder.markdown('<div class="progress-step">ğŸ“– æ­¥éª¤3: è§£æè®ºæ–‡ä¸­...</div>', unsafe_allow_html=True)
        
        try:
            parser = PDFParser()
            parsed_papers = parser.batch_parse_papers(papers_dir, extracted_dir)
            
            if not parsed_papers:
                step3_placeholder.markdown('<div class="progress-step error-step">âŒ æ­¥éª¤3: è®ºæ–‡è§£æå¤±è´¥</div>', unsafe_allow_html=True)
                return None
            
            step3_placeholder.markdown(f'<div class="progress-step success-step">âœ… æ­¥éª¤3: æˆåŠŸè§£æ {len(parsed_papers)} ç¯‡è®ºæ–‡</div>', unsafe_allow_html=True)
            
            # è·å–å·²ä¸‹è½½çš„PDFæ–‡ä»¶åˆ—è¡¨ï¼Œç”¨äºæå–æ ‡é¢˜
            pdf_files = list(papers_dir.glob("*.pdf"))
            
            # ä¿å­˜è§£æç»“æœä¸ºJSON
            for i, paper in enumerate(parsed_papers):
                full_text = getattr(paper, "full_text", "")
                if not full_text and hasattr(paper, "sections"):
                    full_text = "\n".join(getattr(section, "text", "") for section in paper.sections)
                
                # è·å–æ ‡é¢˜ï¼Œå¦‚æœæ²¡æœ‰æ ‡é¢˜åˆ™ä»PDFæ–‡ä»¶åä¸­æå–
                paper_title = getattr(paper, "title", "")
                if not paper_title and i < len(pdf_files):
                    # ä»PDFæ–‡ä»¶åæå–æ ‡é¢˜
                    pdf_file = pdf_files[i]
                    file_name = pdf_file.name.replace(".pdf", "")
                    # æ¸…ç†æ–‡ä»¶åï¼Œå»æ‰ArXiv IDéƒ¨åˆ†
                    parts = file_name.split("_", 1)
                    if len(parts) > 1:
                        paper_title = parts[1].replace("_", " ")
                    else:
                        paper_title = file_name.replace("_", " ")
                
                # å¦‚æœä»ç„¶æ²¡æœ‰æ ‡é¢˜ï¼Œå°è¯•ä»å†…å®¹å¼€å¤´æå–
                if not paper_title or paper_title.startswith("Paper_"):
                    lines = full_text.split('\n')[:15]  # æ£€æŸ¥å‰15è¡Œ
                    for line in lines:
                        line = line.strip()
                        if line and len(line) > 10 and len(line) < 200:
                            # å¯èƒ½æ˜¯æ ‡é¢˜ï¼Œæ’é™¤å¸¸è§çš„ç« èŠ‚æ ‡é¢˜
                            line_lower = line.lower()
                            if not any(keyword in line_lower for keyword in 
                                     ['abstract', 'introduction', 'method', 'result', 'conclusion', 
                                      'references', 'appendix', 'acknowledgment', 'figure', 'table']):
                                paper_title = line
                                break
                
                # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
                if not paper_title:
                    paper_title = f"Paper_{i+1}"
                
                parsed_json = {
                    "title": paper_title,
                    "full_text": full_text
                }
                
                # ä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶å
                safe_title = "".join(c for c in paper_title if c.isalnum() or c in (' ', '-', '_')).strip()[:50]
                if not safe_title:
                    safe_title = f"paper_{i+1}"
                
                output_json = extracted_dir / f"{safe_title.replace(' ', '_')}_parsed.json"
                with open(output_json, "w", encoding="utf-8") as f:
                    json.dump(parsed_json, f, ensure_ascii=False, indent=2)
                    
                st.write(f"ğŸ’¾ ä¿å­˜è§£æç»“æœ: {paper_title}")
            
        except Exception as e:
            step3_placeholder.markdown(f'<div class="progress-step error-step">âŒ æ­¥éª¤3: è§£æå¤±è´¥ - {str(e)}</div>', unsafe_allow_html=True)
            return None
        
        # æ­¥éª¤4: æå–åˆ›æ–°ç‚¹
        step4_placeholder = st.empty()
        step4_placeholder.markdown('<div class="progress-step">ğŸ’¡ æ­¥éª¤4: æå–åˆ›æ–°ç‚¹ä¸­...</div>', unsafe_allow_html=True)
        
        try:
            extractor = InnovationExtractor()
            
            json_files = list(extracted_dir.glob("*_parsed.json"))
            extracted_innovations = []
            
            st.write(f"ğŸ” æ‰¾åˆ° {len(json_files)} ä¸ªè§£ææ–‡ä»¶")
            
            for i, json_file in enumerate(json_files, 1):
                try:
                    st.write(f"ğŸ“„ å¤„ç†æ–‡ä»¶ {i}/{len(json_files)}: {json_file.name}")
                    
                    with open(json_file, "r", encoding="utf-8") as f:
                        paper_data = json.load(f)
                    
                    paper_title = paper_data.get("title", "")
                    paper_content = paper_data.get("full_text", "")
                    
                    if not paper_title:
                        st.write(f"âš ï¸ æ–‡ä»¶ {json_file.name} ç¼ºå°‘æ ‡é¢˜ï¼Œè·³è¿‡")
                        continue
                        
                    if not paper_content:
                        st.write(f"âš ï¸ æ–‡ä»¶ {json_file.name} ç¼ºå°‘å†…å®¹ï¼Œè·³è¿‡")
                        continue
                    
                    st.write(f"ğŸ¤– æ­£åœ¨æå–åˆ›æ–°ç‚¹: {paper_title[:50]}...")
                    
                    extracted = extractor.extract_innovations(paper_content, paper_title)
                    if extracted:
                        extracted_innovations.append(extracted)
                        
                        # ä¿å­˜åˆ›æ–°ç‚¹
                        output_file = innovations_dir / f"{paper_title.replace(' ', '_')[:50]}_innovations.json"
                        extractor.save_innovations(extracted, output_file)
                        
                        st.write(f"âœ… æˆåŠŸæå– {len(extracted.innovations)} ä¸ªåˆ›æ–°ç‚¹")
                    else:
                        st.write(f"âŒ æå–å¤±è´¥: {paper_title[:50]}")
                        
                except Exception as file_error:
                    st.write(f"âŒ å¤„ç†æ–‡ä»¶ {json_file.name} æ—¶å‡ºé”™: {str(file_error)}")
                    continue
            
            if not extracted_innovations:
                step4_placeholder.markdown('<div class="progress-step error-step">âŒ æ­¥éª¤4: åˆ›æ–°ç‚¹æå–å¤±è´¥ - æ²¡æœ‰æˆåŠŸæå–ä»»ä½•åˆ›æ–°ç‚¹</div>', unsafe_allow_html=True)
                return None
            
            total_innovations = sum(len(ext.innovations) for ext in extracted_innovations)
            step4_placeholder.markdown(f'<div class="progress-step success-step">âœ… æ­¥éª¤4: æˆåŠŸæå– {total_innovations} ä¸ªåˆ›æ–°ç‚¹</div>', unsafe_allow_html=True)
            
        except Exception as e:
            step4_placeholder.markdown(f'<div class="progress-step error-step">âŒ æ­¥éª¤4: æå–å¤±è´¥ - {str(e)}</div>', unsafe_allow_html=True)
            return None
        
        # æ­¥éª¤5: ç”Ÿæˆæ–°æƒ³æ³•
        step5_placeholder = st.empty()
        step5_placeholder.markdown('<div class="progress-step">ğŸš€ æ­¥éª¤5: ç”Ÿæˆæ–°æƒ³æ³•ä¸­...</div>', unsafe_allow_html=True)
        
        try:
            generator = IdeaGenerator()
            
            result = generator.generate_ideas_from_innovations(extracted_innovations, search_query)
            
            if not result:
                step5_placeholder.markdown('<div class="progress-step error-step">âŒ æ­¥éª¤5: æƒ³æ³•ç”Ÿæˆå¤±è´¥</div>', unsafe_allow_html=True)
                return None
            
            step5_placeholder.markdown(f'<div class="progress-step success-step">âœ… æ­¥éª¤5: æˆåŠŸç”Ÿæˆ {len(result.generated_ideas)} ä¸ªæ–°æƒ³æ³•</div>', unsafe_allow_html=True)
            
            return {
                'search_query': search_query,
                'papers': papers,
                'extracted_innovations': extracted_innovations,
                'generated_ideas': result,
                'session_dir': session_dir
            }
            
        except Exception as e:
            step5_placeholder.markdown(f'<div class="progress-step error-step">âŒ æ­¥éª¤5: ç”Ÿæˆå¤±è´¥ - {str(e)}</div>', unsafe_allow_html=True)
            return None


def display_innovation_results(result: Dict):
    """æ˜¾ç¤ºåˆ›æ–°ç‚¹ç»“æœ"""
    st.markdown("---")
    st.header("ğŸ¯ åˆ›æ–°ç‚¹åˆ†æç»“æœ")
    
    # ç»Ÿè®¡ä¿¡æ¯
    papers = result['papers']
    extracted_innovations = result['extracted_innovations']
    generated_ideas = result['generated_ideas']
    
    total_innovations = sum(len(ext.innovations) for ext in extracted_innovations)
    
    # é¡¶éƒ¨ç»Ÿè®¡å¡ç‰‡
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown(f"""
        <div class="metric-card">
            <h3>ğŸ“„ {len(papers)}</h3>
            <p>è®ºæ–‡æ•°é‡</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="metric-card">
            <h3>ğŸ’¡ {total_innovations}</h3>
            <p>åˆ›æ–°ç‚¹æ•°é‡</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div class="metric-card">
            <h3>ğŸš€ {len(generated_ideas.generated_ideas)}</h3>
            <p>æ–°æƒ³æ³•æ•°é‡</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        avg_novelty = sum(idea.novelty_score for idea in generated_ideas.generated_ideas) / len(generated_ideas.generated_ideas)
        st.markdown(f"""
        <div class="metric-card">
            <h3>â­ {avg_novelty:.2f}</h3>
            <p>å¹³å‡æ–°é¢–æ€§</p>
        </div>
        """, unsafe_allow_html=True)
    
    # åˆ›æ–°ç‚¹å±•ç¤º
    st.subheader("ğŸ’¡ æå–çš„åˆ›æ–°ç‚¹")
    
    for i, extracted in enumerate(extracted_innovations, 1):
        with st.expander(f"ğŸ“„ è®ºæ–‡ {i}: {extracted.paper_title}"):
            st.markdown(f"**ğŸ“ è®ºæ–‡æ‘˜è¦:** {extracted.summary}")
            st.markdown(f"**ğŸ”¢ åˆ›æ–°ç‚¹æ•°é‡:** {len(extracted.innovations)}")
            
            for j, innovation in enumerate(extracted.innovations, 1):
                st.markdown(f"""
                <div class="innovation-card">
                    <h4>ğŸ’¡ åˆ›æ–°ç‚¹ {j}: {innovation.title}</h4>
                    <p><strong>ğŸ“‹ æè¿°:</strong> {innovation.description}</p>
                    <p><strong>ğŸ·ï¸ ç±»åˆ«:</strong> {innovation.category}</p>
                    <p><strong>ğŸ¯ å½±å“:</strong> {innovation.impact}</p>
                    <p><strong>ğŸ”¬ æ–¹æ³•:</strong> {innovation.methodology}</p>
                    <div style="display: flex; gap: 20px; margin-top: 10px;">
                        <span><strong>â­ æ–°é¢–æ€§:</strong> {innovation.novelty_score:.2f}</span>
                        <span><strong>ğŸ¯ ç½®ä¿¡åº¦:</strong> {innovation.confidence:.2f}</span>
                    </div>
                </div>
                """, unsafe_allow_html=True)
    
    # ç”Ÿæˆçš„æ–°æƒ³æ³•
    st.subheader("ğŸš€ ç”Ÿæˆçš„æ–°æƒ³æ³•")
    
    for i, idea in enumerate(generated_ideas.generated_ideas, 1):
        st.markdown(f"""
        <div class="idea-card">
            <h4>ğŸš€ æƒ³æ³• {i}: {idea.title}</h4>
            <p><strong>ğŸ“‹ æè¿°:</strong> {idea.description}</p>
            <p><strong>ğŸ”— æ¥æºåˆ›æ–°ç‚¹:</strong> {', '.join(idea.source_innovations)}</p>
            <p><strong>ğŸ”„ ç»„åˆç±»å‹:</strong> {idea.combination_type}</p>
            <p><strong>ğŸ›¤ï¸ å®æ–½è·¯å¾„:</strong> {idea.implementation_path}</p>
            
            <div style="display: flex; gap: 20px; margin: 10px 0;">
                <span><strong>â­ æ–°é¢–æ€§:</strong> {idea.novelty_score:.2f}</span>
                <span><strong>ğŸ¯ å¯è¡Œæ€§:</strong> {idea.feasibility_score:.2f}</span>
                <span><strong>ğŸš€ å½±å“æ½œåŠ›:</strong> {idea.impact_potential:.2f}</span>
            </div>
            
            <div style="margin-top: 15px;">
                <strong>ğŸ”¬ ç ”ç©¶æ–¹å‘:</strong>
                <ul>
                    {''.join(f'<li>{direction}</li>' for direction in idea.research_directions)}
                </ul>
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    # åˆ†ææ€»ç»“
    st.subheader("ğŸ“Š åˆ†ææ€»ç»“")
    st.markdown(f"""
    <div class="innovation-card">
        <h4>ğŸ¯ æ€»ä½“åˆ†æ</h4>
        <p>{generated_ideas.analysis_summary}</p>
    </div>
    """, unsafe_allow_html=True)


def save_session_results(result: Dict, session_dir: Path, search_query: str):
    """ä¿å­˜ä¼šè¯ç»“æœ"""
    try:
        results_dir = session_dir / "results"
        results_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        output_file = results_dir / f"complete_results_{search_query.replace(' ', '_')}.json"
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_result = {
            "search_query": search_query,
            "timestamp": datetime.now().isoformat(),
            "papers": [
                {
                    "title": paper.title,
                    "authors": paper.authors,
                    "abstract": paper.abstract,
                    "categories": paper.categories,
                    "arxiv_id": paper.arxiv_id
                }
                for paper in result['papers']
            ],
            "innovations": [
                {
                    "paper_title": ext.paper_title,
                    "summary": ext.summary,
                    "innovations": [
                        {
                            "title": inn.title,
                            "description": inn.description,
                            "category": inn.category,
                            "impact": inn.impact,
                            "methodology": inn.methodology,
                            "novelty_score": inn.novelty_score,
                            "confidence": inn.confidence
                        }
                        for inn in ext.innovations
                    ]
                }
                for ext in result['extracted_innovations']
            ],
            "generated_ideas": {
                "topic": result['generated_ideas'].topic,
                "analysis_summary": result['generated_ideas'].analysis_summary,
                "ideas": [
                    {
                        "title": idea.title,
                        "description": idea.description,
                        "source_innovations": idea.source_innovations,
                        "combination_type": idea.combination_type,
                        "feasibility_score": idea.feasibility_score,
                        "novelty_score": idea.novelty_score,
                        "impact_potential": idea.impact_potential,
                        "implementation_path": idea.implementation_path,
                        "research_directions": idea.research_directions
                    }
                    for idea in result['generated_ideas'].generated_ideas
                ]
            }
        }
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(serializable_result, f, ensure_ascii=False, indent=2)
        
        st.success(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        st.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")


def show_history_sidebar():
    """æ˜¾ç¤ºå†å²è®°å½•ä¾§è¾¹æ """
    data_dir = settings.DATA_DIR
    if not data_dir.exists():
        st.info("æš‚æ— å†å²è®°å½•")
        return
    
    # æŸ¥æ‰¾ä¼šè¯ç›®å½•
    session_dirs = [d for d in data_dir.iterdir() 
                   if d.is_dir() and d.name.startswith("session_")]
    
    if not session_dirs:
        st.info("æš‚æ— å†å²è®°å½•")
        return
    
    # æŒ‰æ—¶é—´æ’åº
    session_dirs.sort(key=lambda x: x.name, reverse=True)
    
    st.write("æœ€è¿‘çš„æœç´¢:")
    for session_dir in session_dirs[:5]:  # æ˜¾ç¤ºæœ€è¿‘5æ¬¡
        results_dir = session_dir / "results"
        if results_dir.exists():
            result_files = list(results_dir.glob("complete_results_*.json"))
            if result_files:
                try:
                    with open(result_files[0], 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    timestamp = session_dir.name.replace("session_", "")
                    query = data.get("search_query", "æœªçŸ¥")
                    
                    if st.button(f"ğŸ“… {timestamp[:8]}\nğŸ” {query[:20]}...", key=f"history_{timestamp}"):
                        st.session_state.selected_history = result_files[0]
                        st.rerun()
                        
                except Exception:
                    continue


def show_analysis_page():
    """ç»“æœåˆ†æé¡µé¢"""
    st.header("ğŸ“Š ç»“æœåˆ†æ")
    
    if 'selected_history' in st.session_state:
        # æ˜¾ç¤ºé€‰ä¸­çš„å†å²ç»“æœ
        display_historical_results(st.session_state.selected_history)
    else:
        st.info("è¯·å…ˆåœ¨ä¾§è¾¹æ é€‰æ‹©å†å²è®°å½•ï¼Œæˆ–åœ¨'ä¸€é”®ç”Ÿæˆ'é¡µé¢ç”Ÿæˆæ–°çš„ç»“æœ")


def display_historical_results(result_file: Path):
    """æ˜¾ç¤ºå†å²ç»“æœ"""
    try:
        with open(result_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        st.success(f"å·²åŠ è½½ç»“æœ: {result_file.name}")
        
        # åŸºæœ¬ä¿¡æ¯
        st.subheader("ğŸ“‹ åŸºæœ¬ä¿¡æ¯")
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric("æœç´¢ä¸»é¢˜", data.get('search_query', 'N/A'))
            st.metric("è®ºæ–‡æ•°é‡", len(data.get('papers', [])))
        
        with col2:
            st.metric("åˆ›æ–°ç‚¹æ•°é‡", sum(len(inn['innovations']) for inn in data.get('innovations', [])))
            st.metric("ç”Ÿæˆæƒ³æ³•æ•°é‡", len(data.get('generated_ideas', {}).get('ideas', [])))
        
        # å¯è§†åŒ–åˆ†æ
        st.subheader("ğŸ“Š å¯è§†åŒ–åˆ†æ")
        
        ideas = data.get('generated_ideas', {}).get('ideas', [])
        if ideas:
            # åˆ›å»ºæ•°æ®æ¡†
            df = pd.DataFrame(ideas)
            
            # æ•£ç‚¹å›¾
            fig1 = px.scatter(
                df, x='novelty_score', y='feasibility_score',
                title='æƒ³æ³•åˆ†å¸ƒï¼šæ–°é¢–æ€§ vs å¯è¡Œæ€§',
                hover_data=['title'],
                color='impact_potential',
                color_continuous_scale='viridis'
            )
            st.plotly_chart(fig1, use_container_width=True)
            
            # ç»„åˆç±»å‹åˆ†å¸ƒ
            if 'combination_type' in df.columns:
                type_counts = df['combination_type'].value_counts()
                fig2 = px.pie(
                    values=type_counts.values,
                    names=type_counts.index,
                    title='æƒ³æ³•ç»„åˆç±»å‹åˆ†å¸ƒ'
                )
                st.plotly_chart(fig2, use_container_width=True)
        
    except Exception as e:
        st.error(f"åŠ è½½ç»“æœå¤±è´¥: {e}")


def show_detailed_process_page():
    """è¯¦ç»†æµç¨‹é¡µé¢"""
    st.header("ğŸ“‹ è¯¦ç»†æµç¨‹è¯´æ˜")
    
    st.markdown("""
    ## ğŸ”„ å®Œæ•´å¤„ç†æµç¨‹
    
    ### 1. ğŸ” è®ºæ–‡æœç´¢
    - ä½¿ç”¨ArXiv APIæœç´¢ç›¸å…³è®ºæ–‡
    - æ ¹æ®å…³é”®è¯åŒ¹é…æ ‡é¢˜ã€æ‘˜è¦å’Œå†…å®¹
    - æŒ‰ç›¸å…³æ€§å’Œæ—¶é—´æ’åº
    
    ### 2. ğŸ“¥ è®ºæ–‡ä¸‹è½½
    - ä¸‹è½½é€‰ä¸­è®ºæ–‡çš„PDFæ–‡ä»¶
    - éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
    - å­˜å‚¨åˆ°æœ¬åœ°ç›®å½•
    
    ### 3. ğŸ“– è®ºæ–‡è§£æ
    - ä½¿ç”¨PDFè§£æå™¨æå–æ–‡æœ¬å†…å®¹
    - è¯†åˆ«ç« èŠ‚ç»“æ„å’Œå…³é”®ä¿¡æ¯
    - æ¸…ç†å’Œæ ¼å¼åŒ–æ–‡æœ¬
    
    ### 4. ğŸ’¡ åˆ›æ–°ç‚¹æå–
    - ä½¿ç”¨AIæ¨¡å‹åˆ†æè®ºæ–‡å†…å®¹
    - è¯†åˆ«æŠ€æœ¯åˆ›æ–°ã€æ–¹æ³•åˆ›æ–°å’Œåº”ç”¨åˆ›æ–°
    - è¯„ä¼°åˆ›æ–°ç‚¹çš„æ–°é¢–æ€§å’Œå½±å“åŠ›
    
    ### 5. ğŸš€ æ–°æƒ³æ³•ç”Ÿæˆ
    - åŸºäºæå–çš„åˆ›æ–°ç‚¹è¿›è¡Œç»„åˆåˆ†æ
    - ç”Ÿæˆè·¨é¢†åŸŸçš„æ–°ç ”ç©¶æ–¹å‘
    - è¯„ä¼°å¯è¡Œæ€§å’Œæ½œåœ¨å½±å“
    
    ## ğŸ¯ è¯„ä¼°æŒ‡æ ‡
    
    - **æ–°é¢–æ€§ (Novelty)**: æƒ³æ³•çš„åŸåˆ›æ€§å’Œç‹¬ç‰¹æ€§
    - **å¯è¡Œæ€§ (Feasibility)**: å®ç°çš„æŠ€æœ¯å¯èƒ½æ€§
    - **å½±å“æ½œåŠ› (Impact)**: å¯¹é¢†åŸŸå‘å±•çš„æ½œåœ¨è´¡çŒ®
    - **ç½®ä¿¡åº¦ (Confidence)**: AIæ¨¡å‹çš„åˆ¤æ–­ç½®ä¿¡åº¦
    """)


if __name__ == "__main__":
    main() 